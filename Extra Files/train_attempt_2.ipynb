{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoIIgtkIKPWT"
      },
      "source": [
        "### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qhZv35WKPWZ",
        "outputId": "2735f1da-558d-4c5b-fd8b-7275f5028f43"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import zipfile\n",
        "\n",
        "# Define paths for extraction\n",
        "zip_path_train = '/content/train_dataset.zip'\n",
        "\n",
        "\n",
        "extract_path_train = '/content/train_dataset'\n",
        "\n",
        "\n",
        "# Function to extract a zip file\n",
        "def extract_zip(zip_path, extract_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "\n",
        "# Extract train_dataset.zip\n",
        "extract_zip(zip_path_train, extract_path_train)\n",
        "print(f\"Train dataset extracted to {extract_path_train}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emTwukXVKPWc"
      },
      "source": [
        "### Reading train.csv and Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MFf_f3xKPWd"
      },
      "outputs": [],
      "source": [
        "# Load train.csv\n",
        "train_csv = pd.read_csv(\"train.csv\")  # Replace with the actual path to your train.csv\n",
        "\n",
        "# Mappings for converting labels to integers and vice-versa\n",
        "label_to_int_map = {\n",
        "    'bright dune': 0,\n",
        "    'dark dune': 1,\n",
        "    'spider': 2,\n",
        "    'impact ejecta': 3,\n",
        "    'slope streak': 4,\n",
        "    'swiss cheese': 5,\n",
        "    'crater': 6,\n",
        "    'other': 7\n",
        "}\n",
        "\n",
        "int_to_label_map = {v:k for k, v in label_to_int_map.items()}\n",
        "\n",
        "# Replace labels with integers\n",
        "labels_int = train_csv['Class'].replace(label_to_int_map).tolist()\n",
        "\n",
        "# Construct file paths considering nested structure\n",
        "file_names = train_csv['File Name']\n",
        "new_file_names = [os.path.join(extract_path_train, 'train_dataset', fname) for fname in file_names]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnCmHvkJKPWf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Split into train and test datasets\n",
        "x_train, x_test, y_train, y_test = train_test_split(new_file_names, labels_int, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create DataFrames\n",
        "train_df = pd.DataFrame({'file_path': x_train, 'label': y_train})\n",
        "test_df = pd.DataFrame({'file_path': x_test, 'label': y_test})\n",
        "\n",
        "# Define transformation for images\n",
        "IMAGE_SIZE = (224, 224)\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.TrivialAugmentWide(),\n",
        "    transforms.RandomHorizontalFlip(p=0.6),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "freq_df  = train_df[1].value_counts()\n",
        "print(freq_df)\n",
        "\n",
        "df0 = train_df[train_df[1] == 0].copy()\n",
        "df1 = train_df[train_df[1] == 1].copy()\n",
        "df2 = train_df[train_df[1] == 2].copy()\n",
        "df3 = train_df[train_df[1] == 3].copy()\n",
        "df4 = train_df[train_df[1] == 4].copy()\n",
        "df5 = train_df[train_df[1] == 5].copy()\n",
        "df6 = train_df[train_df[1] == 6].copy()\n",
        "df7 = train_df[train_df[1] == 7].copy()\n",
        "\n",
        "\n",
        "df0 = pd.concat([df0 for k in range(2)], ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
        "df1 = pd.concat([df1 for k in range(4)], ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
        "df2 = pd.concat([df2 for k in range(9)], ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
        "df3 = pd.concat([df3 for k in range(11)], ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
        "df4 = pd.concat([df4 for k in range(4)], ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
        "df5 = pd.concat([df5 for k in range(4)], ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
        "df6 = pd.concat([df6 for k in range(2)], ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
        "df7 = pd.concat([df7 for k in range(1)], ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "\n",
        "train_df_all = pd.concat([df0, df1, df2, df3, df4, df5, df6, df7], ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
        "print(train_df_all[1].value_counts())\n",
        "\n",
        "noel = [600, 350, 300, 250, 600, 400, 1000, 1500]\n",
        "\n",
        "train_df_1 = pd.concat([df0[:noel[0]], \n",
        "                         df1[:noel[1]], \n",
        "                         df2[:noel[2]], \n",
        "                         df3[:noel[3]], \n",
        "                         df4[:noel[4]], \n",
        "                         df5[:noel[5]], \n",
        "                         df6[:noel[6]], \n",
        "                         df7[:noel[7]]], ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "train_df_2 = pd.concat([df0[-noel[0]:], \n",
        "                         df1[-noel[1]:], \n",
        "                         df2[-noel[2]:], \n",
        "                         df3[-noel[3]:], \n",
        "                         df4[-noel[4]:], \n",
        "                         df5[-noel[5]:], \n",
        "                         df6[-noel[6]:], \n",
        "                         df7[-noel[7]:]], ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "train_df_1.shape, train_df_2.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_otxtwhKPWh"
      },
      "source": [
        "### Importing the Images and creation of iterators for training\n",
        "Images are imported as a PyTorch Dataset based on the filenames in the training and testing dataframes and the iterators are initiated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gz2tvpSyKPWi"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Creating a class that will take in the train/test filenames dataframe\n",
        "as input and import and store images as a tuple: (image tensor, integer label)\n",
        "\n",
        "'''\n",
        "# Dataset class for Mars images\n",
        "class MarsImgDataset(Dataset):\n",
        "    def __init__(self, img_df, transform=None):\n",
        "        self.img_df = img_df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_df.iloc[idx, 0]\n",
        "        label = self.img_df.iloc[idx, 1]\n",
        "\n",
        "        image = Image.open(img_path)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPBL15ZnKPWj"
      },
      "outputs": [],
      "source": [
        "train_dataset_1 = MarsImgDataset(train_df_1, data_transform)\n",
        "train_dataset_2 = MarsImgDataset(train_df_2, data_transform)\n",
        "test_dataset = MarsImgDataset(test_df, data_transform)\n",
        "\n",
        "train_loader_1 = DataLoader(dataset=train_dataset_1, batch_size=32, shuffle=True) # Initialise Training Loader\n",
        "train_loader_2 = DataLoader(dataset=train_dataset_2, batch_size=32, shuffle=True) # Initialise Training Loader\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=True) # Initialise Testing Loader\n",
        "\n",
        "# Get a batch of images from the test loader\n",
        "images, labels = next(iter(test_loader))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "kRQ6atQBKPWk",
        "outputId": "359af7d1-59eb-4478-9000-82514d9fb2d0"
      },
      "outputs": [],
      "source": [
        "## Check the dataloaders by plotting the imported images\n",
        "img, label = next(iter(test_loader))\n",
        "figure, axis = plt.subplots(2,2)\n",
        "\n",
        "axis[0,0].imshow(img[0].permute(1,2,0), cmap = 'Greys_r')\n",
        "axis[0,0].set_title(int_to_label_map[label[0].item()])\n",
        "\n",
        "axis[0,1].imshow(img[1].permute(1,2,0), cmap = 'Greys_r')\n",
        "axis[0,1].set_title(int_to_label_map[label[1].item()])\n",
        "\n",
        "axis[1,0].imshow(img[2].permute(1,2,0), cmap = 'Greys_r')\n",
        "axis[1,0].set_title(int_to_label_map[label[2].item()])\n",
        "\n",
        "axis[1,1].imshow(img[3].permute(1,2,0), cmap = 'Greys_r')\n",
        "axis[1,1].set_title(int_to_label_map[label[3].item()])\n",
        "\n",
        "plt.subplots_adjust(\n",
        "    left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.4, hspace=0.4\n",
        ")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "zv0HaZm0_8Lg",
        "outputId": "24e70247-afd8-4a5d-e09b-3e77771c1b55"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIrLjpUXZX1Z"
      },
      "outputs": [],
      "source": [
        "# # Creating a CNN-based image classifier.\n",
        "class ImageClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv_layer_1 = nn.Sequential(\n",
        "          nn.Conv2d(1, 64, 3, padding=1),\n",
        "          nn.ReLU(),\n",
        "          nn.BatchNorm2d(64),\n",
        "          nn.MaxPool2d(2))\n",
        "        self.conv_layer_2 = nn.Sequential(\n",
        "          nn.Conv2d(64, 512, 3, padding=1),\n",
        "          nn.ReLU(),\n",
        "          nn.BatchNorm2d(512),\n",
        "          nn.MaxPool2d(2))\n",
        "        self.conv_layer_3 = nn.Sequential(\n",
        "          nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "          nn.ReLU(),\n",
        "          nn.BatchNorm2d(512),\n",
        "          nn.MaxPool2d(2))\n",
        "        self.classifier = nn.Sequential(\n",
        "          nn.Flatten(),\n",
        "          nn.Linear(in_features=512*3*3, out_features=8))\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.conv_layer_1(x)\n",
        "        x = self.conv_layer_2(x)\n",
        "        x = self.conv_layer_3(x)\n",
        "        x = self.conv_layer_3(x)\n",
        "        x = self.conv_layer_3(x)\n",
        "        x = self.conv_layer_3(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "# Instantiate an object.\n",
        "model = ImageClassifier().to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "load = False #### SET TO TRUE IF YOU HAVE A PRESAVED CHECKPOINT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if load:\n",
        "    checkpoint = torch.load(\"attempt2.pt\")\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1HlpB3x6AQH",
        "outputId": "e2fb691a-bdf7-45f9-a4cb-808ca7005443"
      },
      "outputs": [],
      "source": [
        "# 1. Get a batch of images and labels from the DataLoader\n",
        "img_batch, label_batch = next(iter(train_loader_1))\n",
        "\n",
        "# 2. Get a single image from the batch and unsqueeze the image so its shape fits the model\n",
        "img_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]\n",
        "print(f\"Single image shape: {img_single.shape}\\n\")\n",
        "\n",
        "# 3. Perform a forward pass on a single image\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    pred = model(img_single.to(device))\n",
        "\n",
        "# 4. Print out what's happening and convert model logits -> pred probs -> pred label\n",
        "print(f\"Output logits:\\n{pred}\\n\")\n",
        "print(f\"Output prediction probabilities:\\n{torch.softmax(pred, dim=1)}\\n\")\n",
        "print(f\"Output prediction label:\\n{torch.argmax(torch.softmax(pred, dim=1), dim=1)}\\n\")\n",
        "print(f\"Actual label:\\n{label_single}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQjsvKV9Edzb",
        "outputId": "271c5ce3-d8f8-48a1-833e-66d2b4ac7da2"
      },
      "outputs": [],
      "source": [
        "IMAGE_WIDTH = 224\n",
        "IMAGE_HEIGHT = 224\n",
        "# Install torchinfo if it's not available, import it if it is\n",
        "try:\n",
        "    import torchinfo\n",
        "except:\n",
        "    !pip install torchinfo\n",
        "    import torchinfo\n",
        "\n",
        "from torchinfo import summary\n",
        "# do a test pass through of an example input size\n",
        "summary(model, input_size=[1, 1, IMAGE_WIDTH ,IMAGE_HEIGHT])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WFcr2MbHcy3",
        "outputId": "cb2dbac4-9bac-40e3-c811-600d929de153"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics\n",
        "from torchmetrics import ConfusionMatrix, Precision, Recall, F1Score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHNU3X3ID3BT"
      },
      "outputs": [],
      "source": [
        "from torchmetrics import ConfusionMatrix, Precision, Recall, F1Score\n",
        "\n",
        "def train_step(model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer):\n",
        "    # Put model in train mode\n",
        "    model.train()\n",
        "\n",
        "    # Setup train loss and train accuracy values\n",
        "    train_loss, train_acc = 0, 0\n",
        "\n",
        "     # Define metrics\n",
        "    precision = Precision(average='macro', num_classes=8, task='MULTICLASS').to(device)\n",
        "    recall = Recall(average='macro', num_classes=8, task='MULTICLASS').to(device)\n",
        "    f1 = F1Score(average='macro', num_classes=8, task='MULTICLASS').to(device)\n",
        "\n",
        "    # Loop through data loader data batches\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Send data to target device\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # 1. Forward pass\n",
        "        y_pred = model(X)\n",
        "\n",
        "        # 2. Calculate  and accumulate loss\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # 3. Optimizer zero grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. Loss backward\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate and accumulate accuracy metric across all batches\n",
        "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
        "\n",
        "        # Update metrics\n",
        "        precision.update(y_pred_class, y)\n",
        "        recall.update(y_pred_class, y)\n",
        "        f1.update(y_pred_class, y)\n",
        "\n",
        "\n",
        "    # Adjust metrics to get average loss and accuracy per batch\n",
        "    train_loss = train_loss / len(dataloader)\n",
        "    train_acc = train_acc / len(dataloader)\n",
        "\n",
        "    # Compute precision, recall, f1 score, and confusion matrix\n",
        "    train_precision = precision.compute().item()\n",
        "    train_recall = recall.compute().item()\n",
        "    train_f1 = f1.compute().item()\n",
        "\n",
        "\n",
        "    return train_loss, train_acc, train_precision, train_recall, train_f1\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDlEg0EPD_11"
      },
      "outputs": [],
      "source": [
        "def test_step(model: torch.nn.Module,\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module):\n",
        "    # Put model in eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # Setup test loss and test accuracy values\n",
        "    test_loss, test_acc = 0, 0\n",
        "\n",
        "    # Define metrics\n",
        "    precision = Precision(average='macro', num_classes=8, task='MULTICLASS').to(device)\n",
        "    recall = Recall(average='macro', num_classes=8, task='MULTICLASS').to(device)\n",
        "    f1 = F1Score(average='macro', num_classes=8, task='MULTICLASS').to(device)\n",
        "\n",
        "    # Turn on inference context manager\n",
        "    with torch.inference_mode():\n",
        "        # Loop through DataLoader batches\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "            # Send data to target device\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            # 1. Forward pass\n",
        "            test_pred_logits = model(X)\n",
        "\n",
        "            # 2. Calculate and accumulate loss\n",
        "            loss = loss_fn(test_pred_logits, y)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # Calculate and accumulate accuracy\n",
        "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
        "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
        "\n",
        "            precision.update(test_pred_labels, y)\n",
        "            recall.update(test_pred_labels, y)\n",
        "            f1.update(test_pred_labels, y)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Adjust metrics to get average loss and accuracy per batch\n",
        "    test_loss = test_loss / len(dataloader)\n",
        "    test_acc = test_acc / len(dataloader)\n",
        "\n",
        "     # Compute precision, recall, f1 score, and confusion matrix\n",
        "    test_precision = precision.compute().item()\n",
        "    test_recall = recall.compute().item()\n",
        "    test_f1 = f1.compute().item()\n",
        "\n",
        "\n",
        "    return test_loss, test_acc, test_precision, test_recall, test_f1\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zI-kYnyZJBRE"
      },
      "outputs": [],
      "source": [
        "# Define the train function\n",
        "from tqdm.auto import tqdm\n",
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader_1: torch.utils.data.DataLoader,\n",
        "          train_dataloader_2: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n",
        "          epochs: int = 5,\n",
        "          results = None,\n",
        "          done_epochs = 0):\n",
        "    # Create empty results dictionary\n",
        "    if results == None:\n",
        "        results = {\n",
        "            \"train_loss\": [],\n",
        "            \"train_acc\": [],\n",
        "            \"train_precision\": [],\n",
        "            \"train_recall\": [],\n",
        "            \"train_f1\": [],\n",
        "\n",
        "            \"test_loss\": [],\n",
        "            \"test_acc\": [],\n",
        "            \"test_precision\": [],\n",
        "            \"test_recall\": [],\n",
        "            \"test_f1\": []}\n",
        "\n",
        "\n",
        "    # Loop through training and testing steps for a number of epochs\n",
        "    for epoch in tqdm(range(done_epochs, epochs+done_epochs)):\n",
        "        if epoch%2 == 0:\n",
        "            train_dataloader = train_dataloader_1\n",
        "        else:\n",
        "            train_dataloader = train_dataloader_2\n",
        "\n",
        "        train_loss, train_acc, train_precision, train_recall, train_f1 = train_step(\n",
        "            model=model,\n",
        "            dataloader=train_dataloader,\n",
        "            loss_fn=loss_fn,\n",
        "            optimizer=optimizer)\n",
        "\n",
        "        test_loss, test_acc, test_precision, test_recall, test_f1 = test_step(\n",
        "            model=model,\n",
        "            dataloader=test_dataloader,\n",
        "            loss_fn=loss_fn)\n",
        "\n",
        "        # Print out what's happening\n",
        "        print(\n",
        "            f\"Epoch: {epoch+1} | \"\n",
        "            f\"train_loss: {train_loss:.4f} | \"\n",
        "            f\"train_acc: {train_acc:.4f} | \"\n",
        "            f\"train_precision: {train_precision:.4f} | \"\n",
        "            f\"train_recall: {train_recall:.4f} | \"\n",
        "            f\"train_f1: {train_f1:.4f} | \"\n",
        "            f\"test_loss: {test_loss:.4f} | \"\n",
        "            f\"test_acc: {test_acc:.4f} | \"\n",
        "            f\"test_precision: {test_precision:.4f} | \"\n",
        "            f\"test_recall: {test_recall:.4f} | \"\n",
        "            f\"test_f1: {test_f1:.4f}\"\n",
        "        )\n",
        "\n",
        "        # Update results dictionary\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "        results[\"train_acc\"].append(train_acc)\n",
        "        results[\"train_precision\"].append(train_precision)\n",
        "        results[\"train_recall\"].append(train_recall)\n",
        "        results[\"train_f1\"].append(train_f1)\n",
        "        results[\"test_loss\"].append(test_loss)\n",
        "        results[\"test_acc\"].append(test_acc)\n",
        "        results[\"test_precision\"].append(test_precision)\n",
        "        results[\"test_recall\"].append(test_recall)\n",
        "        results[\"test_f1\"].append(test_f1)\n",
        "\n",
        "        torch.save({\n",
        "                    'model_state_dict':model.state_dict(),\n",
        "                    'opt_state_dict': optimizer.state_dict(),\n",
        "                    'results':results,\n",
        "                    'epoch':epoch\n",
        "                }, \"attempt2.pt\")\n",
        "\n",
        "    # Return the filled results at the end of the epochs\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8df75a2e98c04dbaa31ff98adb3eebc2",
            "43f96fad8c9743bfbc0de6b50d93aaec",
            "c0cc950224a4420aadf30691340802e5",
            "b8601eac6b4843dd8118bda3b0150a1a",
            "cf77bd84e6c941ae96517e3dc7de251c",
            "0afbeb71d933435b8d8e23df92c23a66",
            "61598de4c68443cd8d63c6d1a3c0095d",
            "ab0c7bf846b44397bdd8c2ecb7691237",
            "3d2bedc3650c46c882ff923a1bda69f2",
            "57743fc2585a4824b4a74d18c73d54ea",
            "20f42a9c479e4b9d9e2533924ef1005c"
          ]
        },
        "id": "HLo1LhujJZIr",
        "outputId": "c7680b47-f6a1-4e34-bb75-571aaf5c7617"
      },
      "outputs": [],
      "source": [
        "# Set random seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Set number of epochs\n",
        "NUM_EPOCHS = 60\n",
        "\n",
        "# Setup loss function and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.0002)\n",
        "\n",
        "if load:\n",
        "    optimizer.load_state_dict(checkpoint['opt_state_dict'])\n",
        "    results = checkpoint['results']\n",
        "    done_epochs = checkpoint['epoch']\n",
        "else:\n",
        "    results = None\n",
        "    done_epochs = 0\n",
        "\n",
        "# Start the timer\n",
        "from timeit import default_timer as timer\n",
        "start_time = timer()\n",
        "\n",
        "# Train model_0\n",
        "model_results = train(model=model,\n",
        "                      train_dataloader_1=train_loader_1,\n",
        "                      train_dataloader_2=train_loader_2,\n",
        "                      test_dataloader=test_loader,\n",
        "                      optimizer=optimizer,\n",
        "                      loss_fn=loss_fn,\n",
        "                      epochs=NUM_EPOCHS,\n",
        "                      results=results,\n",
        "                      done_epochs=done_epochs)\n",
        "\n",
        "# End the timer and print out how long it took\n",
        "end_time = timer()\n",
        "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXJYx6hvJ0ED"
      },
      "outputs": [],
      "source": [
        "def plot_loss_curves(results):\n",
        "\n",
        "    results = dict(list(model_results.items()))\n",
        "\n",
        "    # Get the loss values of the results dictionary (training and test)\n",
        "    loss = results['train_loss']\n",
        "    test_loss = results['test_loss']\n",
        "\n",
        "    # Get the accuracy values of the results dictionary (training and test)\n",
        "    accuracy = results['train_acc']\n",
        "    test_accuracy = results['test_acc']\n",
        "\n",
        "    accuracy = results['train_acc']\n",
        "    test_accuracy = results['test_acc']\n",
        "\n",
        "    precision = results['train_precision']\n",
        "    test_precision = results['test_precision']\n",
        "\n",
        "    recall = results['train_recall']\n",
        "    test_recall = results['test_recall']\n",
        "\n",
        "    f1 = results['train_f1']\n",
        "    test_f1 = results['test_f1']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Figure out how many epochs there were\n",
        "    epochs = range(len(results['train_loss']))\n",
        "\n",
        "    # Setup a plot\n",
        "    plt.figure(figsize=(15, 7))\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(2, 3, 1)\n",
        "    plt.plot(epochs, loss, label='train_loss')\n",
        "    plt.plot(epochs, test_loss, label='test_loss')\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(2, 3, 2)\n",
        "    plt.plot(epochs, accuracy, label='train_accuracy')\n",
        "    plt.plot(epochs, test_accuracy, label='test_accuracy')\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend();\n",
        "\n",
        "    # Plot precision\n",
        "    plt.subplot(2, 3, 3)\n",
        "    plt.plot(epochs, precision, label='train_precision')\n",
        "    plt.plot(epochs, test_precision, label='test_precision')\n",
        "    plt.title('Preciison')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot recall\n",
        "    plt.subplot(2, 3, 4)\n",
        "    plt.plot(epochs, recall, label='train_recall')\n",
        "    plt.plot(epochs, test_recall, label='test_recall')\n",
        "    plt.title('Recall')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend();\n",
        "\n",
        "    plt.subplot(2, 3, 5)\n",
        "    plt.plot(epochs, f1, label='train_f1')\n",
        "    plt.plot(epochs, test_f1, label='test_f1')\n",
        "    plt.title('F1 Score')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend();\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "id": "VTM2IvSbJk9V",
        "outputId": "8345aa72-daac-4171-f51c-ad3a4710fbd6"
      },
      "outputs": [],
      "source": [
        "plot_loss_curves(model_results)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0afbeb71d933435b8d8e23df92c23a66": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20f42a9c479e4b9d9e2533924ef1005c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d2bedc3650c46c882ff923a1bda69f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "43f96fad8c9743bfbc0de6b50d93aaec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0afbeb71d933435b8d8e23df92c23a66",
            "placeholder": "​",
            "style": "IPY_MODEL_61598de4c68443cd8d63c6d1a3c0095d",
            "value": "100%"
          }
        },
        "57743fc2585a4824b4a74d18c73d54ea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61598de4c68443cd8d63c6d1a3c0095d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8df75a2e98c04dbaa31ff98adb3eebc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_43f96fad8c9743bfbc0de6b50d93aaec",
              "IPY_MODEL_c0cc950224a4420aadf30691340802e5",
              "IPY_MODEL_b8601eac6b4843dd8118bda3b0150a1a"
            ],
            "layout": "IPY_MODEL_cf77bd84e6c941ae96517e3dc7de251c"
          }
        },
        "ab0c7bf846b44397bdd8c2ecb7691237": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8601eac6b4843dd8118bda3b0150a1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57743fc2585a4824b4a74d18c73d54ea",
            "placeholder": "​",
            "style": "IPY_MODEL_20f42a9c479e4b9d9e2533924ef1005c",
            "value": " 60/60 [1:33:04&lt;00:00, 92.91s/it]"
          }
        },
        "c0cc950224a4420aadf30691340802e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab0c7bf846b44397bdd8c2ecb7691237",
            "max": 60,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3d2bedc3650c46c882ff923a1bda69f2",
            "value": 60
          }
        },
        "cf77bd84e6c941ae96517e3dc7de251c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
